{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Import the Encoder class from transformers_scratch.py\n",
    "from transformers_scratch import Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        pad_idx,\n",
    "        embed_size=512,\n",
    "        num_layers=6,\n",
    "        forward_expansion=4,\n",
    "        heads=8,\n",
    "        dropout=0.1,\n",
    "        device=\"cuda\",\n",
    "        max_length=512\n",
    "    ):\n",
    "        super(SentimentTransformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length\n",
    "        )\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 2)  # 2 classes for sentiment\n",
    "        )\n",
    "\n",
    "        self.pad_idx = pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        encoder_out = self.encoder(src, src_mask)\n",
    "        # Global average pooling over sequence length\n",
    "        pooled = encoder_out.mean(dim=1)\n",
    "        return self.classifier(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDBDataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, vocab, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "        self.label_map = {'positive': 1, 'negative': 0}  # Map labels to integers\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.data[idx]['review'], self.data[idx]['sentiment']\n",
    "        tokens = self.tokenizer(text)[:self.max_length]\n",
    "        # Convert tokens to indices\n",
    "        indices = [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]\n",
    "        label = self.label_map[label]  # Convert label to integer\n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    text_list, label_list = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        label_list.append(_label)\n",
    "        text_list.append(_text)\n",
    "    return pad_sequence(text_list, padding_value=1, batch_first=True), torch.tensor(label_list)\n",
    "\n",
    "def build_vocab(data, tokenizer, max_vocab_size=25000):\n",
    "    counter = Counter()\n",
    "    for example in data:\n",
    "        counter.update(tokenizer(example['review']))\n",
    "    vocab = {word: idx + 2 for idx, (word, _) in enumerate(counter.most_common(max_vocab_size))}\n",
    "    vocab['<pad>'] = 0\n",
    "    vocab['<unk>'] = 1\n",
    "    return vocab\n",
    "\n",
    "def load_data_from_csv(df):\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        data.append({'review': row['review'], 'sentiment': row['sentiment']})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, vocab, tokenizer, save_dir='saved_model'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    # Save model state\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, 'model.pth'))\n",
    "    # Save vocabulary\n",
    "    with open(os.path.join(save_dir, 'vocab.pkl'), 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "    # Save tokenizer\n",
    "    with open(os.path.join(save_dir, 'tokenizer.pkl'), 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    print(f\"Model and artifacts saved to {save_dir}\")\n",
    "\n",
    "def load_model(save_dir='saved_model', device='cpu'):\n",
    "    # Load vocabulary\n",
    "    with open(os.path.join(save_dir, 'vocab.pkl'), 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    # Load tokenizer\n",
    "    with open(os.path.join(save_dir, 'tokenizer.pkl'), 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    # Initialize model\n",
    "    model = SentimentTransformer(\n",
    "        vocab_size=len(vocab),\n",
    "        pad_idx=vocab['<pad>'],\n",
    "        embed_size=64,\n",
    "        num_layers=2,\n",
    "        heads=8,\n",
    "        device=device\n",
    "    ).to(device)\n",
    "    # Load model state\n",
    "    model.load_state_dict(torch.load(os.path.join(save_dir, 'model.pth'), map_location=device))\n",
    "    model.eval()\n",
    "    print(f\"Model and artifacts loaded from {save_dir}\")\n",
    "    return model, vocab, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 10\n",
    "    LEARNING_RATE = 0.001\n",
    "    MAX_LENGTH = 32\n",
    "    EMBED_SIZE = 32\n",
    "    NUM_HEADS = 8\n",
    "    NUM_LAYERS = 2\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load dataset from CSV\n",
    "    df = pd.read_csv('IMDB Dataset.csv', on_bad_lines='skip', quotechar='\"', engine='python')\n",
    "    df_train = df.sample(frac=0.8, random_state=42)\n",
    "    df_test = df.drop(df_train.index)\n",
    "    train_data = load_data_from_csv(df_train)\n",
    "    test_data = load_data_from_csv(df_test)\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = lambda x: x.split()  # Simple whitespace tokenizer\n",
    "\n",
    "    # Build vocabulary\n",
    "    vocab = build_vocab(train_data, tokenizer)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = IMDBDataset(train_data, tokenizer, vocab, MAX_LENGTH)\n",
    "    test_dataset = IMDBDataset(test_data, tokenizer, vocab, MAX_LENGTH)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                            shuffle=True, collate_fn=collate_batch)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                           shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "    # Initialize model\n",
    "    model = SentimentTransformer(\n",
    "        vocab_size=len(vocab),\n",
    "        pad_idx=vocab['<pad>'],\n",
    "        embed_size=EMBED_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        heads=NUM_HEADS,\n",
    "        device=DEVICE\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}')\n",
    "\n",
    "        for batch_idx, (text, labels) in enumerate(progress_bar):\n",
    "            text, labels = text.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(text)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': total_loss/(batch_idx+1)})\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for text, labels in test_loader:\n",
    "                text, labels = text.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(text)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "    # Save the model and artifacts\n",
    "    save_model(model, vocab, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
